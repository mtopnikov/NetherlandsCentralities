{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные опросов населения Нидерландов о мобильности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные опросов населения Нидерландов изначально представлены в формате данных SPSS и являются результатами двух социологических исследований -- MON и OViN. При этом исследование OViN является достаточно стабильным по структуре и набору переменных, в то время как MON является переходной формой исследования от OVD (1985-2003) к OViN (2009 - н.в.)\n",
    "Исходя из изменчивости структуры, а также необходимости перевода нидерландскоязычных данных на русский язык проводилась предварительная подготовка данных. Она залючалась в:\n",
    "\n",
    "- переводе нидерландскоязычных названий переменных в удобный для последующей обработки вид\n",
    "- присвоении текстовых значений на русском языке категориальным переменным, кодированным целыми числами в таблице\n",
    "- приведении имен колонок данных MON и OVIN в единый вид"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт данных и библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyreadstat import pyreadstat\n",
    "from collections import defaultdict, OrderedDict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:/bachelors/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = mapper_columns).to_excel('D:/bachelors/data_venes_renamed_columns.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_MON = 'D:/bachelors/datasets/MobNed_dat/'\n",
    "path_OViN = 'D:/bachelors/datasets/OViN_dat/'\n",
    "\n",
    "l_dir_MON = os.listdir(path_MON)\n",
    "l_dir_OViN = os.listdir(path_OViN)\n",
    "\n",
    "\n",
    "iterator = {\n",
    "    'MON' : {'path' : path_MON,\n",
    "             'list_dir' : l_dir_MON},\n",
    "    'OViN' : {'path' : path_OViN,\n",
    "              'list_dir' : l_dir_OViN}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выгрузка метаданных из таблиц SPSS для последющего перевода и отбора значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables(meta, variables):\n",
    "    \"\"\"\n",
    "    returns metadataframe extracted from MON or OViN data\n",
    "    \"\"\"\n",
    "    current_meta = pd.DataFrame({\n",
    "        'variable' : meta.column_names,\n",
    "        'description' : meta.column_labels\n",
    "    }).assign(\n",
    "        variable = lambda x: x['variable'].str.lower(),\n",
    "        description = lambda x: x['description'].str.lower()\n",
    "    )\n",
    "    current_meta = current_meta[~current_meta.variable.isin(variables.variable)]\n",
    "    print(len(current_meta))\n",
    "    \n",
    "    return current_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(variables_dict):\n",
    "    \"\"\"\n",
    "    returns values labels in dutch language to use in further translation\n",
    "    \"\"\"\n",
    "    values = pd.DataFrame()\n",
    "    for var in tqdm(variables_dict):\n",
    "        current_var_map = variables_dict[var]\n",
    "        values = values.append(pd.DataFrame({\n",
    "            'variable' : [var for i in current_var_map.keys()],\n",
    "            'values' : list(current_var_map.keys()),\n",
    "            'meaning' : list(current_var_map.values())\n",
    "       }))\n",
    "        values['variable'] = values.variable.str.lower()\n",
    "        \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_table = OrderedDict()\n",
    "for data in iterator:\n",
    "    variables_values = OrderedDict()\n",
    "    l_dir = iterator[data]['list_dir']\n",
    "    root_path = iterator[data]['path']\n",
    "    variables = pd.DataFrame(columns = ['variable', 'description'])\n",
    "    \n",
    "    for l in tqdm(l_dir):\n",
    "        meta = pyreadstat.read_sav(root_path + l)[1]\n",
    "        variables = variables.append(get_variables(meta, variables))\n",
    "        \n",
    "        variables_values[l.split('.')[0]] = get_values(meta.variable_value_labels)\n",
    "        \n",
    "    variables_table[data] = variables\n",
    "    \n",
    "    writer = pd.ExcelWriter(f'D:/bachelors/datasets/Variables/{data}_labels.xlsx')\n",
    "    for sheet in variables_values:\n",
    "        variables_values[sheet].to_excel(writer, sheet_name = sheet, index = False)\n",
    "    writer.save()\n",
    "    \n",
    "    \n",
    "writer = pd.ExcelWriter('D:/bachelors/datasets/Variables/variables_descriptions.xlsx')\n",
    "for sheet in variables_table:\n",
    "    variables_table[sheet].to_excel(writer, sheet_name = sheet, index = False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание словарей для переименования и отбора колонок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экспортированные в результате получения метаданных преобразовывались и переводились, в результате чего названия колонок были переведены английский язык для удобства испоользования. После этого началось создание словарей, которые использовались для перевода колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_MON = pd.read_excel(r\"Variables\\variables descriptions_edited.xlsx\", sheet_name = 'MON').dropna()\n",
    "variables_OViN = pd.read_excel(r\"Variables\\variables descriptions_edited.xlsx\", sheet_name = 'OViN').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapper = {\n",
    "    'MON' : dict(zip(variables_MON.variable, variables_MON.eng_name)),\n",
    "    'OViN' : dict(zip(variables_OViN.variable, variables_OViN.eng_name))    \n",
    "}\n",
    "\n",
    "with open('D:/bachelors/datasets/Variables/mappers/columns_mappers.json', 'w') as outfile:\n",
    "    json.dump(columns_mapper, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if `columns_mapper` is not loaded\n",
    "with open ('Variables/mappers/columns_mappers.json') as infile:\n",
    "    columns_mapper = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, meta = pyreadstat.read_sav('OViN_dat/OViN2014_Databestand.sav')\n",
    "df.columns = [i.lower() for i in df.columns]\n",
    "df = df.drop(columns = list(set(df.columns).difference(columns_mapper['OViN'].keys())))\n",
    "df = df.rename(columns = columns_mapper['OViN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_values(values_dict, database_source):\n",
    "    \"\"\"\n",
    "    Selects required columns and maps them with convinient to use names\n",
    "    \"\"\"\n",
    "    new_values = OrderedDict()\n",
    "    for sheet in values_dict:\n",
    "        print(sheet)\n",
    "        current_labels = values_dict[sheet]\n",
    "        new_values[sheet] = current_labels[\n",
    "            current_labels.variable.isin(\n",
    "                columns_mapper[database_source].keys()\n",
    "            )\n",
    "        ].assign(\n",
    "            variable = lambda x: x['variable'].map(columns_mapper[database_source])\n",
    "        )\n",
    "        \n",
    "    return new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databestand MON 2004\n",
      "Databestand MON 2005\n",
      "Databestand MON 2006\n",
      "Databestand MON 2007\n",
      "Databestand MON 2008\n",
      "Databestand MON 2009\n",
      "OViN2010_Databestand\n",
      "OViN2011_Databestand_revisie\n",
      "OViN2012_Databestand_revisie\n",
      "OViN2013_Databestand\n",
      "OViN2014_Databestand\n",
      "OViN2015_Databestand\n",
      "OViN2016_Databestand\n",
      "OViN2017_Databestand\n"
     ]
    }
   ],
   "source": [
    "values_MON_filtered = filter_values(\n",
    "    pd.read_excel('Variables/MON_labels.xlsx', sheet_name = None),\n",
    "    'MON'\n",
    ")\n",
    "\n",
    "values_OViN_filtered = filter_values(\n",
    "    pd.read_excel('Variables/OViN_labels.xlsx', sheet_name = None),\n",
    "    'OViN'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_excel_multi(odict, path):\n",
    "    \"\"\"\n",
    "    Writes an ordered dictionary of dataframes to the multipage excel spreadsheet\n",
    "    \"\"\"\n",
    "    writer = pd.ExcelWriter(path)\n",
    "    for sheet in odict:\n",
    "        odict[sheet].to_excel(writer, sheet_name = sheet, index = False)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_excel_multi(values_MON_filtered, 'Variables/MON_Variables_values_v2.xlsx')\n",
    "write_excel_multi(values_OViN_filtered, 'Variables/OViN_Variables_values_v2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Словари для переименования значений переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "MON_values_map = pd.read_excel('Variables/mappers/MON_Variables_values_to_mappers.xlsx', sheet_name = None)\n",
    "OViN_values_map = pd.read_excel('Variables/mappers/OViN_Variables_values_to_mappers_v2.xlsx', sheet_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_map = {\n",
    "    'MON' : defaultdict(dict),\n",
    "    'OViN' : defaultdict(dict)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet in MON_values_map:\n",
    "    current_mapper = MON_values_map[sheet]\n",
    "    \n",
    "    for var in current_mapper.variable.unique().tolist():\n",
    "        current_variable = current_mapper[current_mapper.variable == var]\n",
    "        values_map['MON'][sheet][var] = dict(zip(current_variable['values'], current_variable['mapped']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet in OViN_values_map:\n",
    "    current_mapper = OViN_values_map[sheet]\n",
    "    \n",
    "    for var in current_mapper.variable.unique().tolist():\n",
    "        current_variable = current_mapper[current_mapper.variable == var]\n",
    "        values_map['OViN'][sheet][var] = dict(zip(current_variable['values'], current_variable['mapped']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Variables/mappers/values_ru_map.json', 'w') as outfile:\n",
    "    json.dump(values_map, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Variables/mappers/values_ru_map.json') as infile:\n",
    "    values_map = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевод данных и их экспорт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После подготовки словарей, был произведен перевод значений переменных внутри таблиц данных и фильтрация колонок, пригодных для использования в качестве статичных годовых слоев с социально-экономическими характеристиками территории. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in iterator:\n",
    "    colnames = columns_mapper[data]\n",
    "    colnames.pop('hsnor', None) # on the fly fixing of mistakes in colnames data (they had two different columns for mopeds)\n",
    "    colnames.pop('hhsnor', None)\n",
    "    colnames.pop('psnor', None)\n",
    "    colnames.pop('opsnor', None)\n",
    "    valuesMap = values_map[data]\n",
    "    l_dir = iterator[data]['list_dir']\n",
    "    root_path = iterator[data]['path']\n",
    "    for l in tqdm(l_dir):\n",
    "        df = pyreadstat.read_sav(root_path + l)[0]\n",
    "        df.columns = [i.lower() for i in df.columns]\n",
    "        \n",
    "        true_keys = list(set(df.columns).intersection(set(colnames.keys())))\n",
    "        mapper = {k : colnames[k] for k in true_keys}\n",
    "        \n",
    "        df = df[list(mapper.keys())].rename(columns = mapper).astype(int, errors = 'ignore')\n",
    "        current_df_values = valuesMap[l.split('.')[0]]\n",
    "\n",
    "        for col in list(current_df_values.keys()):\n",
    "            if (col in ['geo_departure', 'geo_arrival']) and (len(current_df_values[col]) < 100):\n",
    "                df.loc[\n",
    "                    df[col].isin(current_df_values['geo_commune'].keys()), col\n",
    "                ] = df.loc[\n",
    "                    df[col].isin(current_df_values['geo_commune'].keys()), col\n",
    "                ].astype(int).astype(str).map(current_df_values['geo_commune'])\n",
    "            else:\n",
    "                df.loc[\n",
    "                    df[col].isin(current_df_values[col].keys()), col\n",
    "                ] = df.loc[\n",
    "                    df[col].isin(current_df_values[col].keys()), col\n",
    "                ].astype(int).astype(str).map(current_df_values[col])\n",
    "        year = re.findall('\\d+', l)[0]\n",
    "        df.to_csv(f'translated_data/data_{year}.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Геокодирование коммун"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходные данные имеют географическую привязку в виде почтовых индексов, коммун, регионов COROP и провинций Нидерландов. Поскольку в качестве операционной ячейки использовались коммуны, то далее производилось их прямое геокодирование (преобразование адресов в географические координаты)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range(2004, 2018):\n",
    "    datasets[str(y)] = pd.read_csv(f'translated_data/data_{y}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommunes(df):\n",
    "    \"\"\"\n",
    "    Returns the set of unique communes\n",
    "    \"\"\"\n",
    "    list_sets = [set(df.geo_arrival.dropna()), \n",
    "                 set(df.geo_departure.dropna()),\n",
    "                 set(df.geo_commune.dropna())]\n",
    "    return set().union(*list_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "communes_set = set()\n",
    "for dset in datasets:\n",
    "    communes_set = communes_set.union(getCommunes(df))\n",
    "    \n",
    "communes = pd.DataFrame({'commune' : list(communes_set)})\n",
    "communes['commune'] = communes.commune.apply(lambda x: x + ', Netherlands')\n",
    "communes['lat'], communes['lon'] = pd.Series(), pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder\n",
    "from time import sleep\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcode(location):\n",
    "    \"\"\"\n",
    "    Sends request to geocoding server, returns tuple with coordinates\n",
    "    \"\"\"\n",
    "    try:\n",
    "        g = geocoder.arcgis(location).json\n",
    "        return g['lat'], g['lng'], g\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return np.NaN, np.NaN, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for i, row in tqdm(communes.iterrows()):\n",
    "    location = row['commune']\n",
    "    lat, lon, response = gcode(location)\n",
    "    sleep(1)\n",
    "    \n",
    "    responses.append([location, response])\n",
    "    \n",
    "    communes.loc[communes.commune == location, 'lat'] = lat\n",
    "    communes.loc[communes.commune == location, 'lon'] = lon\n",
    "    \n",
    "pd.DataFrame(responses, columns = ['commune', 'response']).to_csv('communes_geocoding_responses_v3.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communes.plot.scatter('lon', 'lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communes[(communes.lat.isna()) | (communes.lon.isna())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_map = {\n",
    "    'lats' : dict(zip(communes.commune, communes.lat)),\n",
    "    'lons' : dict(zip(communes.commune, communes.lon))\n",
    "}\n",
    "\n",
    "with open('Variables/mappers/coordinates_map_v2.json', 'w') as outfile:\n",
    "    json.dump(coordinates_map, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_points_communes = gpd.GeoDataFrame(communes, \n",
    "                                       crs = {'init' : 'epsg:4326'}, \n",
    "                                       geometry = [Point(xy) for xy in zip(communes.lon, communes.lat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_points_communes.to_file('communes_points.gpkg', driver = 'GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение статистики по территориям  на основании данных опроса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опрос содержит некоторые  характеристики домохозяйств, которые являются полезными для использования в дальнейшем ходе исследования, поэтому было принято решение выделить из файлов отдельные статистические слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subsets = {\n",
    "    'sex' : 'dem_sex',\n",
    "    'ageGroups' : 'dem_ageGroup',\n",
    "    'occupation' : 'econ_occupation',\n",
    "    'n_working_hours' : 'econ_work',\n",
    "    'education' : 'social_education',\n",
    "    'income_groups' : 'econ_income',\n",
    "    'income_deciles' : 'econ_hh_income_deciles',\n",
    "    'vehicles' : ['n_cars', 'n_bikes', 'n_motorsycles', 'n_mopeds', 'n_mopeds', 'n_other_vehicles'],\n",
    "    'travel_motivation' : 'motiv_motivation',\n",
    "    'trip_duration' : 'time_travel_time'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datatype in col_subsets:\n",
    "    series_list = []\n",
    "    for year in datasets:\n",
    "        current_dataset = datasets[year]\n",
    "        columns_gr = ['geo_commune']\n",
    "        \n",
    "        if datatype == 'income_groups' and int(year) >= 2010:\n",
    "            subset = 'econ_hh_income'\n",
    "        elif datatype in ['income_deciles', 'trip_duration'] and int(year) < 2010:\n",
    "            continue\n",
    "        else:\n",
    "            subset = col_subsets[datatype]\n",
    "        \n",
    "        \n",
    "        if type(subset) == list:\n",
    "            columns_gr = columns_gr + subset\n",
    "        else:\n",
    "            columns_gr.append(subset)\n",
    "        series_list.append(\n",
    "            current_dataset.groupby(columns_gr)['uid'].nunique()\n",
    "        )\n",
    "    pd.DataFrame(series_list).transpose().to_csv(f'stats_extracted/stats_{datatype}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Получение статистики по коммунам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBS Open Data предоставляет довольно дробную статистику по коммунам Нидерландов вплоть до структуры экономики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbsodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    '2019' : '84583NED',\n",
    "    '2018' : '84286NED',\n",
    "    '2017' : '83765NED',\n",
    "    '2016' : '83487NED',\n",
    "    '2015' : '83220NED',\n",
    "    '2014' : '82931NED',\n",
    "    '2013' : '82339NED'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_map = defaultdict(dict)\n",
    "for year in datasets:\n",
    "    table_id = datasets[year]\n",
    "    pd.DataFrame(\n",
    "        cbsodata.get_data(table_id, typed = True)\n",
    "    ).to_csv(f'territory_bases/database_{year}.csv', sep = ';', index = None)\n",
    "    \n",
    "    \n",
    "    mapping_columns = {}\n",
    "    list_variables = cbsodata.get_meta(table_id, 'DataProperties')\n",
    "    \n",
    "    for var in list_variables:\n",
    "        mapping_columns[var['Key']] = var['Title']\n",
    "        \n",
    "    values_map[year] = mapping_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('territory_bases/map_columns.json', 'w') as outfile:\n",
    "    json.dump(values_map, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('territory_bases/map_columns.json') as infile:\n",
    "    values_map = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('territory_bases/map_columns.xlsx')\n",
    "\n",
    "for year in values_map:\n",
    "    pd.DataFrame(\n",
    "        {'raw_name' : pd.Series(list(values_map[year].keys())),\n",
    "         'name' : pd.Series(list(values_map[year].values()))}\n",
    "    ).to_excel(writer, sheet_name = year, index = False)\n",
    "    \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budgets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "Downloading table '45042NED' failed. 404 Client Error: Not Found for url: https://opendata.cbs.nl/ODataFeed/odata/45042NED/?%24format=json",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cbsodata\\cbsodata3.py\u001b[0m in \u001b[0;36m_download_metadata\u001b[1;34m(table_id, metadata_name, select, filters, catalog_url, proxies)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://opendata.cbs.nl/ODataFeed/odata/45042NED/?%24format=json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8135e6e93977>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbsodata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'45042NED'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cbsodata\\cbsodata3.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(table_id, dir, typed, select, filters, catalog_url, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mcatalog_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_catalog_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cbsodata\\cbsodata3.py\u001b[0m in \u001b[0;36mdownload_data\u001b[1;34m(table_id, dir, typed, select, filters, catalog_url, proxies)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;31m# http://opendata.cbs.nl/ODataApi/OData/37506wwm?$format=json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     metadata_tables = _download_metadata(\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mtable_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatalog_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_catalog_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cbsodata\\cbsodata3.py\u001b[0m in \u001b[0;36m_download_metadata\u001b[1;34m(table_id, metadata_name, select, filters, catalog_url, proxies)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         raise requests.HTTPError(\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;34m\"Downloading table '{}' failed. {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         )\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: Downloading table '45042NED' failed. 404 Client Error: Not Found for url: https://opendata.cbs.nl/ODataFeed/odata/45042NED/?%24format=json"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(cbsodata.get_data('45042NED', typed = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
